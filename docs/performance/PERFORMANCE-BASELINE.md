# Opossum Performance Baseline & Benchmark Results

## Latest Benchmark: 2026-02-12
## Status: ‚úÖ Complete - Production Baseline Validated & Updated
## Version: 1.1.0

**Latest comprehensive analysis:** See `docs/benchmarking/results/20260212/ANALYSIS.md`

---

## üìä Executive Summary (Updated 2026-02-12)

**Opossum's file-based event store delivers excellent performance for event sourcing workloads:**

- **Write:** ~100 events/sec with full durability (fsync) on SSD
- **Read (tag-based, high selectivity):** ~500Œºs for targeted queries  
- **Read (tag-based, 1K events):** ~10ms (sub-linear scaling)
- **Projections:** 15K events/sec rebuild, 9-10Œºs incremental updates (611x faster)
- **Parallel rebuilding:** 2x speedup with 4 CPU cores
- **Descending queries:** Zero overhead (optimized in-place)

**All core operations perform within acceptable ranges for production use.**

---

## üéØ Quick Reference

| Operation | Typical Performance | Best For |
|-----------|-------------------|----------|
| **Single Event Append** | 10.67ms | CRUD operations |
| **Batch Append (5 events)** | 32ms (6.4ms/event) | Bulk imports |
| **Query by Tag** | 0.8ms/1K events | Filtered reads |
| **Query.All()** | 826ms/10K events | Full scans |
| **Projection Rebuild** | 5ms/50 events, 32ms/500 events | Rare full rebuilds |
| **Incremental Projection** | 10-11 Œºs/event | Real-time updates |

---

## üìà Detailed Benchmark Results

### Phase 1: Write Performance (Append)

**Configuration:** Full durability (fsync after every event)

| Benchmark | Time | Throughput | Status |
|-----------|------|------------|--------|
| Single event | 10.67ms | 94 events/sec | ‚úÖ Good |
| Batch 5 events | 32ms total | 156 events/sec | ‚úÖ Good |
| Batch 10 events | 62ms total | 161 events/sec | ‚úÖ Good |
| DCB validation (20 events) | 219ms | 91 events/sec | ‚úÖ Expected |

**Key Findings:**
- Fsync overhead: ~5.6ms per event (unavoidable for durability)
- Batching provides ~66% throughput improvement (94 ‚Üí 156 events/sec)
- DCB validation adds minimal overhead (~2ms per event)

**Recommendation:** Use batch appends when possible (5-10 events optimal)

---

### Phase 2: Read Performance (Queries)

#### A. Tag-Based Queries (Most Common)

| Dataset | Query Time | Per Event | Scaling |
|---------|-----------|-----------|---------|
| 100 events | 80 Œºs | 0.8 Œºs | Baseline |
| 1,000 events | 784 Œºs | 0.78 Œºs | Linear |
| 10,000 events | 2,208 Œºs | 0.22 Œºs | **Sub-linear!** ‚úÖ |

**Result:** Tag queries scale BETTER than linear (0.22x per 10x dataset)

**Why:** Index efficiency + parallel reads

#### B. EventType-Based Queries

| Dataset | Query Time | Per Event | Scaling |
|---------|-----------|-----------|---------|
| 100 events | 131 Œºs | 1.31 Œºs | Baseline |
| 1,000 events | 1,165 Œºs | 1.17 Œºs | Linear |
| 10,000 events | 3,647 Œºs | 0.36 Œºs | **Sub-linear!** ‚úÖ |

**Result:** EventType queries also scale better than linear

#### C. Query.All() Performance

| Dataset | Time | Per Event | Status |
|---------|------|-----------|--------|
| 100 events | 9.5ms | 95 Œºs | ‚úÖ Fast |
| 1,000 events | 84.7ms | 85 Œºs | ‚úÖ Good |
| 10,000 events | 826ms | 83 Œºs | ‚úÖ Acceptable |

**Result:** Linear scaling, acceptable for full scans

**Why slower?** Every event file must be read from disk (not index-based)

#### D. Descending Order Performance (Fixed!)

**Problem Found:** Reversing array after read was 12.56x slower
**Solution:** Reverse positions BEFORE reading files

| Configuration | Time (10K events) | Status |
|--------------|-------------------|--------|
| Before fix | 10,368ms | ‚ùå Broken |
| After fix | 825ms | ‚úÖ Fixed! |
| **Improvement** | **12.56x faster** | üöÄ |

**Result:** Descending queries now have same performance as ascending

---

### Phase 3: Projection Performance

#### A. Projection Rebuild (Full Reconstruction)

| Dataset | Time | Per Event | Scaling |
|---------|------|-----------|---------|
| 50 events | 4.9ms | 98 Œºs | Baseline |
| 250 events | 17.1ms | 68 Œºs | 3.5x (linear) |
| 500 events | 32.7ms | 65 Œºs | 6.7x (linear) |

**Result:** Perfect linear scaling for projection rebuilds

**Memory Usage:** ~7KB per event (manageable)

#### B. Incremental Updates (Real-Time)

| Update Size | Time | vs Full Rebuild |
|-------------|------|----------------|
| +1 event | **10 Œºs** | **487x faster** than 50-event rebuild |
| +10 events | **11 Œºs** | **448x faster** than 50-event rebuild |

**Result:** Incremental updates are VASTLY faster than full rebuilds

**Break-Even Point:** ~40-50 events (incremental faster below, rebuild faster above)

**Recommendation:**
- ‚úÖ Use incremental for <50 new events
- ‚úÖ Use full rebuild for >50 events or stale projections

#### C. Complex Projections (Multiple Event Types)

| Scenario | Time | Status |
|----------|------|--------|
| Multi-type aggregation | 125 Œºs | ‚úÖ Fast |

---

## üèÜ Performance Characteristics

### Strengths

1. **Sub-Linear Read Scaling** ‚úÖ
   - Tag queries: 10x events = 2.8x time
   - EventType queries: 10x events = 3.1x time
   - Excellent for growing datasets

2. **Predictable Write Performance** ‚úÖ
   - Consistent ~10ms per event with fsync
   - Scales linearly with batch size
   - No surprises

3. **Blazing Fast Incremental Projections** ‚úÖ
   - Microsecond-level updates
   - 500x faster than full rebuild
   - Real-time friendly

4. **Efficient Indexing** ‚úÖ
   - Tag-based queries very fast
   - EventType queries optimized
   - Parallel file reads

### Trade-offs

1. **Fsync Overhead** ‚ö†Ô∏è
   - ~5.6ms per event for durability
   - Unavoidable for crash safety
   - Can disable for testing (not recommended for production)

2. **Query.All() Slower** ‚ö†Ô∏è
   - 826ms for 10K events
   - Acceptable for rare full scans
   - Use filtered queries when possible

3. **File-Per-Event Architecture** ‚ÑπÔ∏è
   - Many small files (not one big file)
   - Good: Simple, debuggable
   - Trade-off: Can't batch fsyncs across files

---

## üö´ What We Tried and Removed

### Batched Fsyncs (FAILED)

**Goal:** Batch multiple events ‚Üí single fsync ‚Üí 40-60% improvement

**Result:** **2-2.3x SLOWER** (100-130% worse!)

**Why it failed:**
- Lock contention serialized parallel writes
- File-per-event architecture can't batch fsyncs (each file needs separate syscall)
- Expert advice was for database-style WAL (single file), not file-per-event

**Decision:** Removed entirely

**Lessons Learned:**
1. Expert advice must match YOUR architecture
2. Locks kill concurrent performance
3. Benchmark realistic scenarios
4. Be willing to remove failed features

**Documentation:** See `docs/lessons-learned/batched-flush-failure.md`

**Future:** If >200 events/sec needed, implement WAL-style architecture (see `docs/future-plans/batched-flush-redesign-plan.md`)

---

## üéØ Production Recommendations

### For Write-Heavy Workloads

**Best Practices:**
1. ‚úÖ Use batch appends (5-10 events optimal)
2. ‚úÖ Keep fsync enabled (durability matters)
3. ‚úÖ Use DCB validation for consistency
4. ‚ùå Don't disable fsync in production

**Expected Throughput:** 94-156 events/sec

### For Read-Heavy Workloads

**Best Practices:**
1. ‚úÖ Use tag-based queries when possible (fastest)
2. ‚úÖ Use EventType queries for type filtering
3. ‚úÖ Avoid Query.All() on large datasets
4. ‚úÖ Use projections for complex read models

**Expected Query Time:** <1ms for filtered queries, <100ms for Query.All()

### For Real-Time Projections

**Best Practices:**
1. ‚úÖ Use incremental updates (<50 events)
2. ‚úÖ Rebuild projections overnight (background jobs)
3. ‚úÖ Cache projection results
4. ‚ùå Don't rebuild on every event

**Expected Update Time:** 10-11 Œºs per event

---

## üìä Scaling Characteristics

### How Performance Scales with Dataset Size

| Metric | 10x More Events | Explanation |
|--------|-----------------|-------------|
| Tag Query | **2.8x slower** | Sub-linear (excellent!) |
| EventType Query | **3.1x slower** | Sub-linear (excellent!) |
| Query.All() | **10x slower** | Linear (expected) |
| Projection Rebuild | **10x slower** | Linear (expected) |
| Incremental Update | **Same** | Constant time ‚úÖ |

**Conclusion:** Opossum scales well for filtered queries, linear for full scans

---

## üîß Configuration for Different Scenarios

### Low-Latency (Default)

```csharp
builder.Services.AddOpossum(options =>
{
    options.FlushEventsImmediately = true; // 10ms latency
});
```

**Best for:** CRUD APIs, real-time event processing
**Throughput:** 94 events/sec
**Latency:** ~10ms per event

### Testing (Fast, No Durability)

```csharp
builder.Services.AddOpossum(options =>
{
    options.FlushEventsImmediately = false; // ~3ms latency
});
```

**Best for:** Unit tests, integration tests
**Throughput:** ~300 events/sec
**‚ö†Ô∏è Risk:** Data loss on crash (don't use in production!)

---

## üìà Benchmark Methodology

### Tools & Configuration

**Framework:** BenchmarkDotNet 0.14.0
**Runtime:** .NET 10.0.2 (X64 RyuJIT AVX2)
**Platform:** Windows 11
**Hardware:** Modern SSD

**Benchmark Config:**
- InvocationCount: 1
- UnrollFactor: 1
- Memory Diagnoser: Enabled
- Job: Default (auto-tuned iterations)

### Dataset Sizes

**Write Benchmarks:** 1, 5, 10 events
**Read Benchmarks:** 100, 1K, 10K events
**Projection Benchmarks:** 50, 250, 500 events

**Why these sizes?**
- Representative of real-world workloads
- Demonstrate scaling characteristics
- Avoid memory exhaustion during benchmark runs

### Benchmark Design Principles

1. **Isolate what you measure**
   - Setup in `[IterationSetup]` (not measured)
   - Only benchmark the operation itself

2. **Use realistic scenarios**
   - Test concurrent operations (not just sequential)
   - Include typical query patterns

3. **Measure consistently**
   - Fresh state each iteration
   - No event accumulation across runs

---

## üéì Lessons Learned

### What Worked

1. **Sub-linear read scaling** - Parallel reads + efficient indexing
2. **Incremental projections** - 500x faster than full rebuild
3. **Descending order fix** - Simple Array.Reverse() before read = 12x faster

### What Failed

1. **Batched fsyncs** - Lock contention made it 2x SLOWER
2. **Expert advice without validation** - Database patterns don't fit file-per-event

### Key Takeaways

1. ‚úÖ **Validate expert advice** against YOUR architecture
2. ‚úÖ **Benchmark realistic scenarios** (concurrent, not isolated)
3. ‚úÖ **Locks kill concurrency** (avoid in hot paths)
4. ‚úÖ **Be willing to remove failed features** (sunk cost fallacy is real)
5. ‚úÖ **Simple often wins** (Array.Reverse() > complex algorithms)

---

## üîÆ Future Optimizations

### If We Need >200 Events/Sec

**Option:** WAL-Style Architecture
- Switch from file-per-event to append-only log
- Batch events in single file ‚Üí single fsync
- Expected: 2-3x throughput improvement

**Effort:** 2-3 weeks
**Risk:** Medium (architecture change)

**See:** `docs/future-plans/batched-flush-redesign-plan.md`

### Query Optimizations (Low Priority)

**Query.All() could be faster with:**
- Memory-mapped file reads
- Compressed event storage
- Event caching

**Current performance (826ms/10K) is acceptable for rare full scans**

---

## üìö Related Documentation

### Latest Benchmark Results (2026-02-12)
- **`docs/benchmarking/results/20260212/ANALYSIS.md`** - Comprehensive benchmark analysis
  - Append performance: ~100 events/sec with flush
  - Tag query (high selectivity): ~500Œºs
  - Projection rebuild: ~15K events/sec
  - Parallel rebuilding: 2x speedup with 4 cores
  - Descending order: Zero overhead (optimized!)
  - DCB append: Actually faster than regular append

### Historical Benchmarks
- `docs/lessons-learned/batched-flush-failure.md` - What we tried and why it failed
- `docs/future-plans/batched-flush-redesign-plan.md` - How to implement batching properly

### Implementation Details
- `docs/implementation/durability-guarantees.md` - Fsync and crash safety
- `docs/features/ledger.md` - Sequence position management

### Specifications
- `Specification/DCB-Specification.md` - Dynamic Consistency Boundaries

---

## ‚úÖ Verification Checklist

**Before claiming good performance:**
- [x] All benchmarks run successfully
- [x] Results are consistent (¬±5% variance)
- [x] Memory usage reasonable (<500MB for benchmarks)
- [x] Scaling characteristics understood
- [x] Production recommendations documented

---

## üéØ Summary

**Opossum delivers:**
- ‚úÖ Good write throughput (94-156 events/sec)
- ‚úÖ Excellent read performance (sub-linear scaling)
- ‚úÖ Blazing fast incremental projections (10 Œºs)
- ‚úÖ Predictable, production-ready performance

**Current performance is sufficient for most event sourcing workloads!**

**Don't optimize prematurely. The baseline is good enough. üéâ**

---

**Baseline Established:** 2025-01-28
**Status:** Production Ready  
**Next:** Monitor production workloads, optimize if needed
