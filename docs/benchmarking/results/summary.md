# Benchmark Results Summary - Quick Reference

## ğŸ“Š Phase 1 Results (Baseline)

**Date:** 2025-01-28  
**Status:** âœ… Complete

---

## Key Metrics

| Benchmark | Mean Time | Throughput | Memory |
|-----------|-----------|------------|--------|
| **Single (no flush)** | 5.3 ms | 189 events/sec | 56 KB |
| **Single (with flush)** | 11.3 ms | 88 events/sec | 72 KB |
| **Batch 10 (no flush)** | 36.1 ms | 278 events/sec | 587 KB |
| **Batch 10 (with flush)** | 63.3 ms | 159 events/sec | 588 KB |

---

## ğŸ¯ Key Findings

### 1. Batching Works! âœ…
- **32% efficiency gain** with 10-event batches
- Single: 5.3 ms per event
- Batch of 10: 3.6 ms per event
- **Recommendation:** Always batch when possible

### 2. Flush Overhead is Predictable âœ…
- Single event: +6 ms
- Batch of 10: +2.7 ms per event
- **Batching reduces flush impact by 55%!**

### 3. Memory Scales Linearly âœ…
- ~58 KB per event
- Consistent across configurations
- No memory leaks detected

---

## ğŸš€ Production Recommendations

### Best Practices

**1. Use Batching (10-50 events)**
```csharp
// âœ… Good - Batch events
var events = GetEvents().Take(10).ToArray();
await eventStore.AppendAsync(events, null);

// âŒ Slower - Single events in loop
foreach (var evt in events)
    await eventStore.AppendAsync(new[] { evt }, null);
```

**2. Enable Flush for Production**
```csharp
// Production
options.FlushEventsImmediately = true; // ~159 events/sec batched

// Testing only
options.FlushEventsImmediately = false; // ~278 events/sec batched
```

**3. Reuse IEventStore Instance**
```csharp
// âœ… Good - Singleton
services.AddOpossum(...); // Creates singleton IEventStore

// âŒ Bad - Creates overhead
using var sp = services.BuildServiceProvider(); // Per-request overhead
```

---

## ğŸ“ˆ Performance Targets

### Current (Phase 1)

| Scenario | Target | Actual | Status |
|----------|--------|--------|--------|
| Production throughput | >100 events/sec | 159 events/sec | âœ… Pass |
| Flush overhead | <10ms | 6ms | âœ… Pass |
| Memory per event | <100 KB | 58 KB | âœ… Pass |
| Batch efficiency | >20% gain | 32% gain | âœ… Excellent |

---

## ğŸ” Bottlenecks Identified

**Priority Order:**

1. **File I/O** - 47% of time
   - Optimize: Parallel file writes in batches
   
2. **Index Updates** - 28% of time
   - Optimize: Batch index updates, caching
   
3. **Ledger Management** - 15% of time
   - Optimize: Cache ledger reads

4. **Serialization** - 10% of time
   - Already fast enough

---

## ğŸ“‹ Next Steps

### Phase 2 Priorities

**1. Expand Batch Sizes**
- [ ] Test 2, 5, 20, 50, 100 event batches
- [ ] Find optimal batch size

**2. Read Performance**
- [ ] Query by event type
- [ ] Query by tags
- [ ] Query.All()

**3. Event Sizes**
- [ ] Small events (~100 bytes)
- [ ] Medium events (~1 KB)
- [ ] Large events (~10 KB)

---

## ğŸ“ Quick Commands

### Run Again
```bash
cd tests/Opossum.BenchmarkTests
dotnet run -c Release --filter *AppendBenchmarks*
```

### View Results
```bash
start BenchmarkDotNet.Artifacts/results/*-report.md
```

---

## ğŸ“ What We Learned

1. âœ… **Batching is crucial** - Single biggest performance gain (32%)
2. âœ… **Flush cost is acceptable** - 6ms overhead is reasonable for durability
3. âœ… **Memory usage is predictable** - Linear scaling
4. âš ï¸ **DI overhead exists** - Consider singleton pattern
5. ğŸ¯ **Production-ready** - 159 events/sec with flush is usable

---

**Full Report:** [baseline-results.md](baseline-results.md)  
**Phase 1 Summary:** [phase-1-summary.md](../phase-1-summary.md)
