# Opossum 0.5.0 â€” Release Roadmap

> **Status:** Planning
> **Target release:** 0.5.0
> **Builds on:** 0.4.0 (cross-process append safety â€” ADR-005)

This document captures improvements planned for the 0.5.0 milestone.

Items 2, 3, and 4 (write throughput optimisations A, B, E) were originally scoped for
0.4.0 but were deferred to keep that release tightly focused on the ADR-005 correctness
fix. The full research and design rationale lives in
`docs/analysis/throughput-research-and-improvement-paths.md` and
`docs/future-plans/0.4.0-roadmap.md#2-write-throughput-optimization-ab-e-under-discussion`.

Item 1 (`ReflectionMessageHandler` delegate cache) was also deferred from 0.4.0.

---

## Overview

| # | Feature | Breaking? | Effort |
|---|---------|-----------|--------|
| 1 | [Cache compiled delegate in `ReflectionMessageHandler`](#1-cache-compiled-delegate-in-reflectionmessagehandler) | No | Medium |
| 2 | [Option A â€” Append-only index files](#2-option-a--append-only-index-files) | Yes (index format) | Low |
| 3 | [Option B â€” In-memory index cache](#3-option-b--in-memory-index-cache) | No | Medium |
| 4 | [Option E â€” Implicit ledger](#4-option-e--implicit-ledger) | Yes (ledger removed) | Lowâ€“Medium |
| 5 | [Crash-recovery position collision (WAL problem)](#5-crash-recovery-position-collision) | No | Medium |
| 6 | [Corrupt-ledger silent reset (`GetLastSequencePositionAsync` + `JsonException`)](#6-corrupt-ledger-silent-reset) | No | Small |
| 7 | [`ProjectionTagIndex._tagLocks` grows without bound](#7-projectiontagindex_taglocks-grows-without-bound) | No | Small |
| 8 | [Parallel read threshold of 10 is an undocumented magic number](#8-parallel-read-threshold-magic-number) | No | Trivial |
| 9 | [`LogReadError` receives nullable `StoreName` without null-coalescing guard](#9-logreaderr-nullable-storename) | No | Trivial |

---

## 1. Cache Compiled Delegate in `ReflectionMessageHandler`

### Why deferred from 0.4.0

0.4.0 bandwidth is fully occupied by ADR-005 (cross-process lock) and the A+B+E
storage-layer throughput work. The `ReflectionMessageHandler` optimisation does not
interact with either of those changes and carries non-trivial implementation risk
in edge cases (generic handlers, static handlers, `Task<T>` return types). Deferring
it preserves a focused 0.4.0 scope without any loss of function.

### Current state

`ReflectionMessageHandler.HandleAsync` dispatches every message via `MethodInfo.Invoke`:

```csharp
result = _method.Invoke(handlerInstance, args);
```

`_method` and `_parameters` are stored at construction time, so the handler avoids
repeated `GetMethod` / `GetParameters` lookups. However, `MethodInfo.Invoke` itself
still pays a per-call price:

- **Boxing** â€” value-type arguments are boxed into `object?[]` on every dispatch.
- **Security and visibility checks** â€” the CLR re-validates accessibility on each
  invocation unless the method was explicitly JIT-compiled into an open delegate.
- **`TargetInvocationException` wrapping** â€” exceptions thrown by the handler are
  wrapped and must be explicitly unwrapped, adding a catch block to the hot path.
- **No inlining** â€” the JIT cannot inline through a reflection call site.

For low-throughput mediator usage (e.g. one command per HTTP request) this overhead is
negligible. For high-throughput scenarios â€” batch processing, event replay driving
mediator dispatch, or benchmarks comparing Opossum to other mediator libraries â€” the
`Invoke` overhead becomes a measurable bottleneck.

### Why this matters

Every `await mediator.SendAsync(command)` call currently pays `MethodInfo.Invoke` overhead.
At 10,000 commands/second â€” plausible for a batch-processing use case â€” the per-call
overhead accumulates. A compiled open delegate eliminates boxing, removes the `Invoke`
trampoline, and lets the JIT inline the async state machine setup.

The improvement is especially visible in the replay scenario introduced by 0.3.0's
streaming reads: a projection that dispatches events through the mediator (for side-effect
processing) can hit tens of thousands of `HandleAsync` calls during a rebuild.

### Proposed implementation

Replace `MethodInfo.Invoke` with a compiled open delegate created once at construction
time and cached in a `readonly` field:

```csharp
// Compiled once per handler registration â€” paid at startup, not per-dispatch
private readonly Func<object?, object?[], object?> _compiled;

public ReflectionMessageHandler(Type handlerType, MethodInfo method)
{
    // ... existing null checks and field assignments ...

    _compiled = BuildCompiledDelegate(handlerType, method);
}

private static Func<object?, object?[], object?> BuildCompiledDelegate(
    Type handlerType, MethodInfo method)
{
    // instance parameter (null for static methods)
    var instanceParam = Expression.Parameter(typeof(object), "instance");
    // args array parameter
    var argsParam = Expression.Parameter(typeof(object[]), "args");

    var parameters = method.GetParameters();
    var argExpressions = new Expression[parameters.Length];
    for (int i = 0; i < parameters.Length; i++)
    {
        argExpressions[i] = Expression.Convert(
            Expression.ArrayIndex(argsParam, Expression.Constant(i)),
            parameters[i].ParameterType);
    }

    Expression? instanceExpr = method.IsStatic
        ? null
        : Expression.Convert(instanceParam, handlerType);

    var callExpr = Expression.Call(instanceExpr, method, argExpressions);

    Expression body = method.ReturnType == typeof(void)
        ? Expression.Block(callExpr, Expression.Constant(null, typeof(object)))
        : Expression.Convert(callExpr, typeof(object));

    return Expression.Lambda<Func<object?, object?[], object?>>(
        body, instanceParam, argsParam).Compile();
}
```

`HandleAsync` then replaces `_method.Invoke` with `_compiled(handlerInstance, args)`,
and the `TargetInvocationException` catch block is removed (compiled delegates rethrow
the original exception directly).

### Implementation considerations

- **Expression tree approach** is safer than `Delegate.CreateDelegate` for the general
  case because it handles static methods, instance methods, and different return types
  (`void`, `Task`, `Task<T>`) uniformly.
- **`async Task<T>` result extraction** â€” the existing `resultProperty?.GetValue(task)`
  reflection call for reading `Task<T>.Result` should also be replaced with a compiled
  accessor, or replaced by casting to `dynamic` / using `Task.ContinueWith` with typed
  continuations.
- **Startup cost** â€” `Expression.Compile()` is non-trivial but paid once per handler
  registration (at `AddMediator()` startup), not per dispatch. Acceptable trade-off.
- **Test coverage** â€” the existing `ReflectionMessageHandler` tests must be extended to
  cover static handlers, void-return handlers, and `Task<T>`-return handlers to guard
  against regressions in the compiled-delegate path.

### Breaking impact

None. This is a pure internal implementation change. The public `IMessageHandler` and
`Mediator` APIs are unchanged. Existing handler implementations require no modification.

### Performance expectation

| Scenario | Before | After (estimate) |
|----------|--------|-----------------|
| Single command dispatch | ~2â€“5 Âµs (`Invoke` overhead) | ~0.1â€“0.3 Âµs (delegate call) |
| 10,000 commands/s sustained | measurable CPU in profiler | negligible |
| Projection replay (streaming, 100k events) | `Invoke` in hot loop | inlined delegate |

Actual numbers should be verified with a `ReflectionMessageHandler` micro-benchmark
added to `Opossum.BenchmarkTests` before shipping this change.

---

## 2. Option A â€” Append-only index files

> Deferred from 0.4.0. Full design detail in `docs/future-plans/0.4.0-roadmap.md`
> and the throughput research in `docs/analysis/throughput-research-and-improvement-paths.md`.

### What it is

Replace the current full-rewrite index files (`.json`) with newline-delimited
append-only position lists (`.idx`):

```
# tagkey=tagvalue.idx
1
5
12
47
```

Append = `File.AppendAllTextAsync($"{position}\n")` â€” one write, no read, no temp file.
Read = `File.ReadAllLinesAsync(...)` â€” parse longs, deduplicate on load.

The cross-process lock from ADR-005 guarantees only one appender is active at a time,
making this safe. Concurrent readers use `FileShare.Read`, which is compatible with an
open append handle.

### Breaking change

Index file format changes from `.json` to `.idx`. Existing stores require a one-time
migration on first startup.

### Why ship before production stores exist

This is the only item with a **breaking store format change**. Shipping it while the
installed base is zero eliminates all future migration burden. Each release this is
deferred increases the migration cost for real users.

### Expected gain

~14% improvement on `FlushEventsImmediately = true` path (~92 â†’ ~105 events/sec).

---

## 3. Option B â€” In-memory index cache

> Deferred from 0.4.0. Full design detail in `docs/future-plans/0.4.0-roadmap.md`
> and the throughput research in `docs/analysis/throughput-research-and-improvement-paths.md`.

### What it is

Load all tag and event-type indexes into memory at startup. All read queries and all
DCB condition checks are served from the in-memory cache (nanosecond lookups regardless
of store size). On append (inside the cross-process lock), update the in-memory cache
first, then flush only the M changed `.idx` files to disk before releasing the lock.

Cross-process staleness is detected in O(1) via an `AsOfPosition` counter: on every
lock acquisition the ledger position is already read; if it does not match the cache's
watermark, only the event files written since the last hold are read and only their
M index files are reloaded (typically 2â€“4 files per event).

**Depends on Option A** â€” the cache is keyed on the `.idx` files that Option A introduces.

### Breaking change

None. Transparent behaviour change.

### Expected gain

~19% improvement on top of Option A (~105 â†’ ~125 events/sec on flush=true path).

---

## 4. Option E â€” Implicit ledger

> Deferred from 0.4.0 with decision still **pending**. The decision must be made
> before implementation begins. Full design detail in `docs/future-plans/0.4.0-roadmap.md`.

### What it is

Eliminate the `.ledger` file entirely. Position tracking becomes:

- **In-memory:** a `long` counter incremented inside the cross-process lock (never stale
  because only one writer holds the lock at a time)
- **On startup:** scan the events directory for the highest-numbered `*.json` file to
  initialise the counter
- **Crash recovery:** if the app crashes mid-append the temp file is cleaned up on
  restart; the startup scan finds the correct highest committed position

The second fsync (ledger) disappears. Only the event file fsync remains on the
flush=true path.

### Decision required before implementation

The correctness of replacing an explicit ledger with an implicit directory scan, the
crash-recovery edge cases (partial rename, interrupted rename), and the impact on
store observability must be explicitly approved before this option is confirmed.

### Breaking change

The `.ledger` file is removed from the store directory. Existing stores migrate
transparently â€” on first startup the position is re-derived from the directory scan.

### Expected gain

~48% improvement on top of Options A+B (~125 â†’ ~185 events/sec on flush=true path).

---

## Combined performance expectation (Options A+B+E)

| Configuration | 0.4.0 baseline | After A+B | After A+B+E |
|---|---|---|---|
| No-flush (local) | ~205 events/sec | ~400â€“500 events/sec (~2.4Ă—) | ~550â€“650 events/sec (~3Ă—) |
| Flush=true (local) | ~92 events/sec | ~125â€“130 events/sec (~1.4Ă—) | ~180â€“190 events/sec (~2Ă—) |
| Flush=true (SMB LAN) | ~70â€“80 events/sec | ~90â€“100 events/sec (~1.3Ă—) | ~120â€“140 events/sec (~1.7Ă—) |

---

## 5. Crash-Recovery Position Collision

> **Type:** Bug / Known Limitation
> **Severity:** High — silent data loss possible
> **Introduced:** v0.1.0
> **Documentation:** [docs/limitations/crash-recovery-position-collision.md](../limitations/crash-recovery-position-collision.md)

### Root cause

AppendAsync writes in three sequential phases, with the position becoming "committed"
only at the very last step:

- **Step 7** — Write event files at positions N, N+1, …
- **Step 8** — Update index files
- **Step 9** — Update ledger ‹ only here does the position become "committed"

A process crash between steps 7 and 9 leaves event files on disk at positions the ledger
does not yet know about. The next GetNextSequencePositionAsync reads the stale ledger,
allocates the same positions again, and WriteEventAsync silently overwrites the orphaned
event files — even with WriteProtectEventFiles = true, because the write path explicitly
strips the ReadOnly attribute before every overwrite. This attribute-strip was introduced
for the legitimate AddTagsAsync maintenance path but leaves crash-orphaned files
equally unguarded.

### Why deferred from 0.4.x

The correct fix (ledger-first write or idempotent existence check) requires careful design
to avoid introducing new races with the cross-process lock introduced in ADR-005. The crash
window is very short (milliseconds per append), the condition requires an unclean process
termination, and there are zero production stores on v0.4.x — making this an acceptable
known risk for the preview release series. Documenting it explicitly ensures it is
addressed before any production recommendation.

### Proposed fix

**Option B — existence check before overwrite (preferred for 0.5.0)**

Before calling File.Move in EventFileManager.WriteEventAsync, check
File.Exists(filePath). If the destination already exists, skip the write (the previous
run already persisted the event). After all event files are confirmed on disk, reconcile
the ledger to the highest persisted position before proceeding to the index update.

This makes every per-event write **idempotent** on restart without any file-format change.

**Option A — ledger-first (WAL semantics, future consideration)**

Move the ledger update to before step 7 so it records intent before event files are
written. A startup recovery scan detects positions in the ledger that lack corresponding
event files and re-runs the write. More principled but more complex; deferred unless
Option B proves insufficient. Implementation decision to be recorded in a new ADR.

### Acceptance criteria

- [ ] WriteEventAsync detects and skips an already-present event file rather than silently overwriting it.
- [ ] The ledger is reconciled to the actual highest on-disk position after crash-recovery.
- [ ] A unit test simulates step-7-only completion (event file written, ledger not updated) and verifies no silent overwrite on the next append.
- [ ] An integration test verifies that re-running AppendAsync after a simulated mid-write failure produces correct, non-overlapping positions.
- [ ] All existing tests continue to pass.
- [ ] No change to the public IEventStore API.

---

## 6. Corrupt-Ledger Silent Reset

> **Type:** Reliability / Latent Bug
> **Severity:** Catastrophic if triggered — silent event overwrite
> **Introduced:** v0.1.0
> **Location:** `src/Opossum/Storage/FileSystem/LedgerManager.cs` — `GetLastSequencePositionAsync`

### Root cause

`GetLastSequencePositionAsync` catches `JsonException` and returns `0`:

```csharp
catch (JsonException)
{
    // Ledger is corrupt - return 0 and let it be rebuilt
    return 0;
}
```

`GetNextSequencePositionAsync` then returns `0 + 1 = 1`. The next `AppendAsync` call
allocates positions starting at 1 again and silently overwrites every committed event
file from the beginning of the store — even with `WriteProtectEventFiles = true`, because
the write path strips the `ReadOnly` attribute before every overwrite (see also Ticket 5).

### Why acceptable for preview

The atomic temp+rename write strategy used by `UpdateSequencePositionAsync` means the
ledger file transitions from the previous valid JSON to the new valid JSON with no
intermediate partial state. A corrupt ledger therefore requires either:

- hardware-level bit rot (extremely rare; undetectable without checksums regardless), or
- direct manual file modification.

The practical probability of a corrupt ledger arising from normal operation is negligible
for the preview release series with no production stores.

### Proposed fix

Replace the silent `return 0` with an explicit throw:

```csharp
catch (JsonException ex)
{
    throw new InvalidOperationException(
        $"Ledger file at '{ledgerPath}' is corrupt and cannot be parsed. " +
        "Delete or repair the ledger file before restarting the store.", ex);
}
```

This transforms a silent catastrophic overwrite into a loud, actionable failure that
surfaces the corruption immediately rather than destroying data.

A recovery utility (out of scope for this ticket) can optionally be added to `IEventStoreAdmin`
to re-derive the correct position from the events directory — identical to the startup
scan proposed in Option E (Ticket 4).

### Acceptance criteria

- [ ] `GetLastSequencePositionAsync` throws `InvalidOperationException` (not `JsonException`) when the ledger file contains invalid JSON.
- [ ] The exception message includes the ledger file path and actionable recovery guidance.
- [ ] A unit test verifies the new exception is thrown rather than `0` being returned.
- [ ] No change to the public `IEventStore` API.
- [ ] All existing tests continue to pass.

---

## 7. `ProjectionTagIndex._tagLocks` Grows Without Bound

> **Type:** Memory / Resource Leak
> **Severity:** Low — bounded by the application's tag-value cardinality
> **Introduced:** v0.3.0
> **Location:** `src/Opossum/Projections/ProjectionTagIndex.cs`

### Root cause

`ProjectionTagIndex` uses a `ConcurrentDictionary<string, SemaphoreSlim>` to provide
per-tag locking granularity so concurrent writes to different tags do not contend:

```csharp
private readonly ConcurrentDictionary<string, SemaphoreSlim> _tagLocks = new();

var semaphore = _tagLocks.GetOrAdd(tagLockKey, _ => new SemaphoreSlim(1, 1));
```

Entries are added on first access and never removed. Each entry holds a live
`SemaphoreSlim` object (small but non-zero managed + unmanaged resources). Over the
lifetime of a long-running process the dictionary grows monotonically — one entry per
distinct `(projectionPath, tagKey, tagValue)` triple ever seen.

### Why acceptable for the target use case

Business tag sets are typically bounded and stable (e.g. `status=active|archived`,
`region=EU|US`). In those scenarios the dictionary reaches its maximum size at first
full population and never grows further. The per-entry overhead (~100 bytes) is
negligible relative to the event store's file I/O cost.

The issue only becomes relevant for high-cardinality tags (e.g. tagging by arbitrary
user-supplied free-text or GUIDs) combined with very long process uptimes.

### Proposed fix

Replace the unbounded dictionary with a bounded-capacity LRU cache, or switch to a
striped-lock strategy (a fixed-size array of `SemaphoreSlim`s indexed by `tagLockKey.GetHashCode() % stripeCount`):

```csharp
// Fixed memory footprint; no dictionary needed
private static readonly SemaphoreSlim[] _stripes =
    Enumerable.Range(0, 64).Select(_ => new SemaphoreSlim(1, 1)).ToArray();

private static SemaphoreSlim GetStripe(string key) =>
    _stripes[(uint)key.GetHashCode() % (uint)_stripes.Length];
```

Striped locking increases the probability of false contention (two different tags sharing
a stripe), but at 64 stripes the probability is low and the memory is completely fixed.
The optimal stripe count should be validated against the projection rebuild benchmark.

### Acceptance criteria

- [ ] `ProjectionTagIndex` no longer accumulates unbounded `SemaphoreSlim` entries.
- [ ] The chosen replacement strategy (striped lock or LRU) is documented with rationale.
- [ ] Existing `TagIndexThreadSafetyTests` continue to pass without modification.
- [ ] A comment in the source explains the chosen approach and its tradeoffs.

---

## 8. Parallel Read Threshold Magic Number

> **Type:** Code Quality / Maintainability
> **Severity:** Harmless — arbitrary value but no correctness impact
> **Introduced:** v0.3.0
> **Location:** `src/Opossum/Storage/FileSystem/EventFileManager.cs` — `ReadEventsAsync`

### Root cause

`ReadEventsAsync` switches from sequential to `Parallel.ForEachAsync` at exactly
10 positions:

```csharp
if (positions.Length < 10)
{
    // sequential path
}
// parallel path
```

The value `10` was not derived from benchmarks. It is an undocumented heuristic that
has never been verified against actual I/O profiling data. Its presence as a bare
literal makes it invisible during code review and impossible to tune without modifying
source code.

### Proposed fix

Extract the threshold to a named private constant with a clear description of what it
represents and a reference to the benchmark that should validate it:

```csharp
/// <summary>
/// Minimum number of events required to justify the overhead of
/// <see cref="Parallel.ForEachAsync{TSource}"/>. Below this threshold sequential
/// reads are faster due to lower task-scheduling cost.
/// Value derived from: <c>Opossum.BenchmarkTests.ReadEventsBenchmark</c> (TODO: add benchmark).
/// </summary>
private const int ParallelReadThreshold = 10;
```

Ideally a dedicated `ReadEventsBenchmark` is added to `Opossum.BenchmarkTests` to
measure the crossover point on representative hardware (both HDD and SSD) and
replace the constant with the empirically correct value.

### Acceptance criteria

- [ ] The literal `10` in `ReadEventsAsync` is replaced with a named constant `ParallelReadThreshold`.
- [ ] The constant carries an XML doc comment explaining its purpose and referencing the benchmark.
- [ ] A `ReadEventsBenchmark` is added to `Opossum.BenchmarkTests` covering sequential vs. parallel reads at batch sizes 1, 5, 10, 20, 50, 100, 500.
- [ ] All existing tests continue to pass.

---

## 9. `LogReadError` Receives Nullable `StoreName` Without Null-Coalescing Guard

> **Type:** Code Quality / Correctness
> **Severity:** Zero runtime impact — logs correctly due to nullable annotation coercion
> **Introduced:** v0.2.0
> **Location:** `src/Opossum/Storage/FileSystem/FileSystemEventStore.cs` — `ReadAsync`

### Root cause

`LogReadError` is declared with a non-nullable `string` parameter:

```csharp
[LoggerMessage(Level = LogLevel.Error, Message = "Error reading events from context '{Context}'")]
private partial void LogReadError(Exception ex, string context);
```

The call site passes `_options.StoreName`, which is typed `string?`:

```csharp
LogReadError(ex, _options.StoreName);
```

This is a nullable-to-non-nullable assignment. The compiler emits no warning only because
the source generator for `LoggerMessage` silently accepts the nullable value, but it is
a latent annotation mismatch. The symmetric `LogAppendError` call already uses the
correct null-coalescing pattern:

```csharp
LogAppendError(ex, _options.StoreName ?? string.Empty);
```

### Proposed fix

Apply the same null-coalescing guard that `LogAppendError` already uses:

```csharp
LogReadError(ex, _options.StoreName ?? string.Empty);
```

This makes both call sites consistent, eliminates the annotation mismatch, and is a
single-character-class change with zero behaviour impact.

### Acceptance criteria

- [ ] `LogReadError(ex, _options.StoreName)` is changed to `LogReadError(ex, _options.StoreName ?? string.Empty)`.
- [ ] Build produces `0 Warning(s)`.
- [ ] All existing tests continue to pass.
