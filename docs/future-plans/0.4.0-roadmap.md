# Opossum 0.4.0 — Release Roadmap

> **Status:** ✅ Complete — ADR-005 delivered; write throughput optimisations (A, B, E) deferred to 0.5.0
> **Target release:** 0.4.0
> **Builds on:** 0.3.0-preview.1 (`Apply(SequencedEvent)`, immutable record types, `AddTagsAsync`)

This document captures improvements planned for the 0.4.0 milestone.

ADR-005 — the multi-user correctness fix that unlocks the core SMB deployment scenario —
was the only **hard release requirement** for 0.4.0 (see [Release Gate Analysis](#release-gate-analysis)).
It has been implemented and all 824 tests pass.

The write throughput optimisations (Options A, B, and E) were originally scoped alongside
ADR-005, but the Release Gate Analysis confirmed they are not required for production
correctness or for the target use cases. They have been deferred to 0.5.0 to keep this
release tightly scoped. See `docs/future-plans/0.5.0-roadmap.md` for the 0.5.0 plan.

The `ReflectionMessageHandler` compiled-delegate optimisation was also deferred to 0.5.0.
See `docs/future-plans/0.5.0-roadmap.md`.

---

## Overview

| # | Feature | Breaking? | Effort | Status |
|---|---------|-----------|--------|--------|
| 1 | [Cross-Process Append Safety (ADR-005)](#1-cross-process-append-safety-adr-005) | No | Medium | ✅ Implemented |
| 2 | [Write Throughput Optimization (A, B, E)](#2-write-throughput-optimization-ab-e-under-discussion) | Yes (A: index format; E: ledger TBD) | Medium | ⏭️ Deferred to 0.5.0 |

---

## 1. Cross-Process Append Safety (ADR-005) ✅ Implemented

> **Delivered in 0.4.0.** Full decision record: `docs/decisions/005-cross-process-append-safety.md`

### Current state (historical)

`FileSystemEventStore.AppendAsync` uses a `SemaphoreSlim(1, 1)` to serialise concurrent
appends. A `SemaphoreSlim` is a **process-local** primitive. When two application
instances on separate machines both call `AppendAsync` simultaneously, each acquires its
own semaphore independently, creating a read-check-write race that can silently overwrite
events at the same sequence position.

This is not a theoretical concern: it is a certainty any time two PCs submit a form within
the same ~10 ms window on a shared network drive — exactly the multi-workstation deployment
profile Opossum targets.

### Proposed implementation

Introduce a **dedicated `.store.lock` file** in the context directory, opened with
`FileShare.None` for the full duration of every append operation. The combined strategy:

```
SemaphoreSlim.WaitAsync()                    ← existing; fast within-process gate
  Acquire .store.lock (FileShare.None)        ← new; cross-process gate
    ValidateAppendCondition
    ReadLedger
    WriteEventFiles
    UpdateIndexes
    UpdateLedger
  Release .store.lock (close FileStream)
SemaphoreSlim.Release()
```

The `SemaphoreSlim` is kept to prevent all threads within the same process from
simultaneously contesting the OS file lock.

Windows SMB enforces `FileShare.None` **server-side, across all machines on the network**.
When PC1 holds the lock file open, the SMB server rejects PC2's open attempt with
`ERROR_SHARING_VIOLATION`. The OS releases all file handles on process termination —
even on unclean exit — so there is no stale-lock scenario.

### New class: `CrossProcessLockManager`

New file: `src/Opossum/Storage/FileSystem/CrossProcessLockManager.cs`

Responsibilities:
- Acquire `.store.lock` with `FileShare.None` and exponential-backoff retry on
  `IOException` with sharing-violation HResult
- Throw `TimeoutException` when `CrossProcessLockTimeout` is exceeded
- Return an `IAsyncDisposable` handle that releases the lock on disposal

### New option: `OpossumOptions.CrossProcessLockTimeout`

```csharp
/// <summary>
/// Maximum time to wait when acquiring the cross-process append lock.
/// Default: 5 seconds.
/// </summary>
public TimeSpan CrossProcessLockTimeout { get; set; } = TimeSpan.FromSeconds(5);
```

No opt-in flag. The lock acquisition on an uncontested file is a single kernel syscall
(~0.1 ms) — immeasurable against the ~10 ms append with `FlushEventsImmediately = true`.

### Performance impact

| Path | Lock overhead | Total append time | Impact |
|------|--------------|-------------------|--------|
| Local drive | ~0.1 ms | ~10 ms | < 1% |
| SMB share (LAN) | ~1–4 ms | ~12–14 ms | ~10–30% |

At the actual event rates of the target use cases (< 2 events/sec across all PCs), this
overhead is completely imperceptible to users.

### Breaking impact

None. `CrossProcessLockTimeout` is a new additive option with a safe default.
No public API surface changes. No store directory format changes.

### Test coverage

**Unit tests** (`Opossum.UnitTests`):

| Test | What it verifies |
|------|-----------------|
| `AcquiresLock_WhenFileIsNotHeld` | Handle returned; `.store.lock` exists on disk |
| `SecondAcquire_Fails_WhenLockIsHeld` | Open lock file manually; `AcquireAsync` must throw `TimeoutException` |
| `ReleasesLock_OnDispose` | Acquire → dispose → acquire again must succeed immediately |
| `RespectsCancellationToken` | Already-cancelled token throws `OperationCanceledException` immediately |
| `CancellationMidWait_Throws` | Cancel mid-backoff throws `OperationCanceledException` |

**Integration tests** (`Opossum.IntegrationTests`):

Simulate cross-process contention within a single process by holding the `.store.lock`
file open directly from the test while a concurrent `AppendAsync` call races against it.

---

## 2. Write Throughput Optimization (A, B, E) ⏭️ Deferred to 0.5.0

> **Deferred from 0.4.0.** The Release Gate Analysis confirmed these optimisations are
> not required for correctness or for any documented target use case throughput ceiling.
> The full plan has been moved to `docs/future-plans/0.5.0-roadmap.md`.
> The detailed analysis remains in `docs/analysis/throughput-research-and-improvement-paths.md`.

Full analysis: `docs/analysis/throughput-research-and-improvement-paths.md`

### Current state

From `AppendBenchmarks` (SSD, Windows 11, .NET 10):

| Scenario | Mean | Throughput |
|---|---|---|
| Single event, `FlushEventsImmediately = false` | 4.874 ms | ~205 events/sec |
| Single event, `FlushEventsImmediately = true` | 10.853 ms | ~92 events/sec |

The bottleneck decomposes into two independent problems:

1. **No-flush path (~4.9 ms):** The **read-modify-write pattern on index files** — every
   append reads the full `tagkey=tagvalue.json` and `EventTypeIndex` files, rewrites them
   from scratch, and renames the temp file. For 2 tags this is ~62% of the no-flush cost.

2. **Flush path (+6 ms):** Two independent fsyncs — event file (~3.5 ms) and ledger
   (~2.5 ms). The ledger fsync is redundant: the event file's fsync-before-rename already
   guarantees the position is durable (encoded in the filename).

### Three targeted fixes

#### Option A — Append-only index files · Low effort · ~1.5× on no-flush path

Replace full-rewrite index files with newline-delimited append-only position lists:

```
# tagkey=tagvalue.idx  (was: tagkey=tagvalue.json)
1
5
12
47
```

Append = `File.AppendAllTextAsync($"{position}\n")` — one write, no read, no temp file.  
Read = `File.ReadAllLinesAsync(...)` — parse longs, deduplicate on load.

The cross-process lock from ADR-005 guarantees that only one appender is active at a
time, making this safe. Concurrent readers use `FileShare.Read`, which is compatible
with an open append handle.

**Breaking change:** Index file format changes. Existing stores need a one-time migration
on first startup.

#### Option B — In-memory index cache · Medium effort · ~1.4× on no-flush path

**Architecture (two-tier):**

```
In-memory cache (per-process)   ← always current; used for DCB validation and reads
On-disk .idx files              ← flushed before releasing the cross-process lock
```

Load all tag and event-type indexes into memory at startup. All read queries and all
DCB condition checks are served from the in-memory cache — **nanosecond lookups**
regardless of store size. On append (inside the cross-process lock), update the
in-memory cache first, then flush only the M changed `.idx` files to disk before
releasing the lock.

**Why indexes cannot be deferred to a background job:**

DCB `ValidateAppendConditionAsync` calls `IndexManager.GetPositionsByTagAsync` for
every tag in the `AppendCondition.FailIfEventsMatch` query, **inside the exclusive
lock**. If the index were eventually consistent, a process could read a stale tag list
and pass a condition that should have failed — silently violating a business invariant.
The alternatives are:

| Approach | DCB check latency (1 k events) | DCB check latency (10 k events) | Correct? |
|---|---|---|---|
| Current — disk read per tag | ~2–4 ms | ~2–4 ms | ✅ |
| Option B — in-memory lookup | ~0 ms | ~0 ms | ✅ |
| Background indexer, no cache | — | — | ❌ stale reads |
| Fallback to event file scan | ~1,000 ms | ~10,000 ms | ✅ but O(N) |

The on-disk flush before lock release remains on the critical path, but only for
M ≤ (tags per event + 1 event type) files — typically 3–4 append-only writes.

**Cross-process staleness detection — O(1) via position counter:**

The in-memory cache stamps itself with `AsOfPosition` — the store position at the time
the cache was last updated. On every lock acquisition the ledger is already read to
allocate the next position (it is on the critical path regardless). The staleness check
is a free integer comparison on top of that existing read:

```
Lock acquired
  currentPosition = ReadLedger()              ← already needed; 0 extra cost
  if currentPosition == cache.AsOfPosition:
      cache is fresh → 0 additional disk reads ← common case: this process last wrote
  else:
      for pos in (cache.AsOfPosition + 1)..currentPosition:
          event = ReadEventFile(pos)           ← O(delta), typically 1 file
          collect event.Tags + event.EventType
      reload only the M affected .idx files   ← O(M) = 2–4 files
      cache.AsOfPosition = currentPosition
```

`delta` = events appended by other processes since this process last held the lock.
In the target deployment (< 2 events/sec shared across all PCs) `delta` is 0 or 1
in the overwhelming majority of acquisitions — zero extra disk reads in the
uncontested single-workstation case, one event file read + 2–4 index reloads in the
multi-PC contested case.

**Breaking change:** None. Format unchanged; behaviour change is transparent.

#### Option E — Implicit ledger · Low–Medium effort · ~1.4× on flush=true path ⚠️ Under discussion — not yet decided

Eliminate the ledger file entirely. Position tracking becomes:
- **In-memory:** a `long` counter incremented inside the cross-process lock (never stale
  because only one writer holds the lock at a time)
- **On startup:** scan the events directory for the highest-numbered `*.json` file to
  initialise the counter
- **Crash recovery:** if the app crashes mid-append the temp file is cleaned up on
  restart; the startup scan finds the correct highest committed position

The second fsync (ledger) disappears. Only the event file fsync remains.

**Breaking change:** The `.ledger` file disappears from the store directory. Existing
stores migrate transparently — on first startup the position is re-derived from the
directory scan.

> ⚠️ **Decision pending.** Option E has not been approved for 0.4.0. The correctness
> of replacing an explicit ledger with an implicit directory scan, the crash-recovery
> edge cases, and the impact on store observability need to be discussed before this
> option is confirmed or deferred.

### Combined performance expectation

| Configuration | Before | After A+B (confirmed) | After A+B+E (if E approved) |
|---|---|---|---|
| No-flush (local) | ~205 events/sec | ~400–500 events/sec (~2.4×) | ~550–650 events/sec (~3×) |
| Flush=true (local) | ~92 events/sec | ~125–130 events/sec (~1.4×) | ~180–190 events/sec (~2×) |
| Flush=true (SMB LAN) | ~70–80 events/sec | ~90–100 events/sec (~1.3×) | ~120–140 events/sec (~1.7×) |

The flush=true ceiling with file-per-event + one fsync is ~285 events/sec. Breaking
through that ceiling requires batched fsyncs (group commit) or Option C (WAL) —
both deferred to a future milestone.

### Implementation order

ADR-005 must land first. Options A and B can be implemented once ADR-005 lands; Option B
depends on A (the cache is keyed on the same `.idx` files that A introduces).
Option E is excluded from the confirmed order until the decision is made.

| Step | Gain on flush=true | Status |
|------|-------------------|--------|
| ADR-005 | Correctness (not speed) | ✅ Implemented in 0.4.0 |
| Option A | ~92 → ~105/sec (+14%) | ⏭️ Deferred to 0.5.0 |
| Option B | ~105 → ~125/sec (+19%) | ⏭️ Deferred to 0.5.0 |
| Option E | ~125 → ~185/sec (+48%) | ⏭️ Deferred to 0.5.0 (decision pending) |

### Breaking impact summary

| Change | Breaking? | Migration | Status |
|--------|-----------|-----------|--------|
| ADR-005 | No | None | ✅ Implemented in 0.4.0 |
| Option A (index format) | Yes | One-time migration on first startup | ⏭️ Deferred to 0.5.0 |
| Option B (in-memory cache) | No | None | ⏭️ Deferred to 0.5.0 |
| Option E (implicit ledger) | Yes | Transparent — startup scan re-derives position | ⏭️ Deferred to 0.5.0 (decision pending) |

### Test coverage

- Extend existing `AppendBenchmarks` with post-A+B scenarios to verify the ~1.4×
  throughput improvement on `FlushEventsImmediately = true`.
- Add migration tests: open a store written by the old format, verify all events are
  readable and new appends succeed.
- **Option B — staleness detection tests** (simulating cross-process writes within a
  single test process by writing `.idx` files directly and advancing the ledger):
  - `Cache_IsNotReloaded_WhenPositionUnchanged` — acquire lock twice with no intervening
    write; verify zero index file reads on second acquisition
  - `Cache_ReloadsOnlyAffectedFiles_WhenOtherProcessWrote` — write an event file and
    advance the ledger externally; verify only the 2–4 tag/type files for that event
    are reloaded, not the full index set
  - `DcbCondition_SeesExternallyWrittenEvent_AfterCacheReload` — write an event that
    satisfies an `AppendCondition` externally; the next `AppendAsync` with that condition
    must throw `ConcurrencyException`, not silently pass
- If Option E is approved: verify crash-recovery behaviour by simulating process
  termination after rename but before in-memory counter update; confirm restart
  recovers correctly.

---

## Release Gate Analysis

### ADR-005 is the only hard release requirement

The current throughput ceiling (`FlushEventsImmediately = true`, production default) is
~92 events/sec. The event rates of every documented target use case are far below this:

| Use case | Peak event rate | Headroom at 92 events/sec |
|---|---|---|
| ISO 9001 quality management | < 0.1/sec | **920×** |
| Pharmacy / controlled substance dispensing | < 0.5/sec | **184×** |
| GLP/GMP laboratory records | < 0.1/sec | **920×** |
| Professional services billing | < 0.01/sec | **9,200×** |
| HACCP food production records | < 0.05/sec | **1,840×** |
| Developer tooling / internal audit | < 1/sec | **92×** |

92 events/sec with full durability is not a constraint for any use case in the target
market. **Throughput alone does not block production deployment.**

What does block production deployment is **correctness**: without ADR-005, two
concurrent application instances on separate machines can silently overwrite each
other's events. That makes Opossum functionally single-user regardless of any other
capability. ADR-005 is the gate.

### Per-item gate classification

| Item | Release gate? | Outcome |
|------|---------------|---------|
| ADR-005 (cross-process lock) | ✅ Hard requirement | ✅ Implemented |
| Option A (append-only index format) | ⚠️ Opportunistic | ⏭️ Deferred to 0.5.0 |
| Option B (in-memory index cache) | ❌ Deferrable | ⏭️ Deferred to 0.5.0 |
| Option E (implicit ledger) | ❌ Deferrable | ⏭️ Deferred to 0.5.0 (decision pending) |

### Why Option A is cheapest to ship now rather than later

Option A is the only item with a **breaking store format change** (index files change
from `.json` to `.idx`). The cost of a breaking change is proportional to the number
of existing production stores that must be migrated.

- Shipped **before** first production stores exist → zero stores to migrate; cost is
  purely implementation time.
- Shipped **after** production stores exist → every deployed store requires a one-time
  migration on first startup; migration logic must be maintained indefinitely.

This is not a throughput argument. It is a *cheapest-moment-to-break* argument: if
Option A is going to ship at all, shipping it as part of 0.4.0 alongside ADR-005
(before production data exists) eliminates all future migration burden at no extra cost
to users. Slipping it to 0.5.0 creates migration work that would otherwise not exist.

Option B carries no format change and can slip to 0.5.0 with zero consequences.

### 0.5.0 deferral — final outcome

| Item | Outcome | Reason |
|------|---------|--------|
| Option A (append-only index format) | ⏭️ Deferred to 0.5.0 | Not a correctness gate; 0 production stores to migrate yet — cheapest moment to land is 0.5.0 |
| Option B (in-memory index cache) | ⏭️ Deferred to 0.5.0 | No format impact; pure runtime optimisation |
| Option E (implicit ledger) | ⏭️ Deferred to 0.5.0 | Decision still pending; not a correctness gate |
| `ReflectionMessageHandler` delegate cache | ⏭️ Deferred to 0.5.0 | See `docs/future-plans/0.5.0-roadmap.md` |
